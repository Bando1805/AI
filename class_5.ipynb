{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18gia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "with open('names.txt', 'r') as f:\n",
    "    words = f.read().split('\\n')\n",
    "#characters mapping\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} \n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(chars)+1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    global block_size\n",
    "    X , Y = [],[]\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        context = [0] * block_size\n",
    "        #print(word)\n",
    "        for ch in word + '.':\n",
    "            X.append(context)\n",
    "            ix = stoi[ch]\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context),'--->',itos[ix])\n",
    "            #update context\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3453, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([32, 3]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n),Yb] = -1.0/n\n",
    "dprobs = (1/probs) * dlogprobs \n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim = 1,keepdim = True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = - counts_sum ** -2 * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "dlogits = torch.ones_like(logits) *  dnorm_logits\n",
    "dlogit_maxes = - (torch.ones_like(logits)* dnorm_logits).sum(dim = 1,keepdim= True) \n",
    "max_col_idxs = logits.argmax(dim = 1)\n",
    "b = torch.zeros_like(logits)\n",
    "b[range(n),max_col_idxs] = 1.0\n",
    "dlogits += b * dlogit_maxes\n",
    "dh = dlogits @ torch.t(W2)\n",
    "dW2 = torch.t(h) @ dlogits\n",
    "db2 = (torch.ones_like(h @ W2) *dlogits).sum(dim=0)\n",
    "dhpreact = (1 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(dim = 0, keepdim = True)\n",
    "dbnraw = torch.concat([bngain for _ in range(bnraw.shape[0])],0) * dhpreact\n",
    "dbnbias = (torch.ones_like(hpreact) * dhpreact).sum(dim = 0, keepdim = True)\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(dim = 0, keepdim = True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5  * dbnvar_inv\n",
    "dbndiff2 = 1/(n-1)*(torch.ones_like(bndiff2)) * dbnvar \n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "dbnmeani = - (torch.ones_like(bndiff) * dbndiff).sum(dim= 0, keepdim = True) \n",
    "dhprebn = torch.ones_like(hprebn) * dbndiff\n",
    "dhprebn += (1/n)*torch.ones_like(hprebn)*dbnmeani\n",
    "dembcat = dhprebn @ torch.t(W1)\n",
    "dW1 = torch.t(embcat) @ dhprebn\n",
    "db1 = (torch.ones_like(hprebn) * dhprebn).sum(dim = 0)\n",
    "demb = (torch.ones_like(embcat) * dembcat).view(emb.shape[0],emb.shape[1],emb.shape[2])\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        idx = Xb[i,j]\n",
    "        dC[idx] += demb[i,j]\n",
    "\n",
    "C.shape, Xb.shape, emb.shape\n",
    "#emb = C[Xb] # embed the characters into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.423782229423523e-09\n"
     ]
    }
   ],
   "source": [
    "#softmax derivation\n",
    "dlogits = F.softmax(logits,1)\n",
    "dlogits[range(n),Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0698, 0.0937, 0.0179, 0.0552, 0.0190, 0.0830, 0.0249, 0.0319, 0.0161,\n",
       "        0.0310, 0.0411, 0.0329, 0.0359, 0.0311, 0.0337, 0.0128, 0.0079, 0.0201,\n",
       "        0.0162, 0.0527, 0.0470, 0.0194, 0.0244, 0.0743, 0.0583, 0.0283, 0.0213],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0698,  0.0937,  0.0179,  0.0552,  0.0190,  0.0830,  0.0249,  0.0319,\n",
       "        -0.9839,  0.0310,  0.0411,  0.0329,  0.0359,  0.0311,  0.0337,  0.0128,\n",
       "         0.0079,  0.0201,  0.0162,  0.0527,  0.0470,  0.0194,  0.0244,  0.0743,\n",
       "         0.0583,  0.0283,  0.0213], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2596e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()\n",
    "#close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x284d908ded0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.imshow(dlogits.detach(), cmap = 'gray')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dlogits are basically Pi for wrong characters and Pi-1 for correct characters.\n",
    "We have no to imagine the neural network as a system of pulls. If the corect character has Pi = 1 and the other charcater have Pi = 0, then all the derivative are 0 and nothing get pulled. This is what we want since the prediction is correct.\n",
    "Meanwhile if all the characters have probability 0 and one character which is wrong has Pi = 1. Then the right characther has dlogits = -1 and the wrong character has dlogits = 1. This means that the probability of the right character will be strongly pulled up while the wrong character will be strongly pulled down. This is what we want since the prediction is wrong. The other wrong characters will have dlogits = 0 and will not be pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dhpreabn = bngain * bnvar_inv/n * (n*dhpreact -dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn',dhprebn,hprebn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "166ec3b1c466d025881b3168ffea78040cf02218c3ecc06e4bd76e23952cc283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
